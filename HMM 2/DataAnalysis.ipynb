{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to C:\\Users\\Kinjalk\n",
      "[nltk_data]     Parth\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\brown.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common prefix: ('ing', 2113)\n",
      "Most common suffix: ('gni', 2113)\n",
      "Most common infix: ('ing', 2113)\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "from collections import Counter\n",
    "\n",
    "def find_common_parts(words, part_length):\n",
    "    parts = Counter()\n",
    "    for word in words:\n",
    "        for i in range(len(word) - part_length + 1):\n",
    "            part = word[i:i+part_length]\n",
    "            parts[part] += 1\n",
    "    return parts.most_common(1)[0]\n",
    "\n",
    "# Ensure the Brown Corpus is downloaded\n",
    "nltk.download('brown')\n",
    "\n",
    "# Get the words from the Brown Corpus\n",
    "words = brown.words()\n",
    "\n",
    "# Find hapax words\n",
    "fdist = nltk.FreqDist(words)\n",
    "hapax_words = fdist.hapaxes()\n",
    "\n",
    "part_length = 3  # replace with your desired part length\n",
    "common_prefix = find_common_parts(hapax_words, part_length)\n",
    "common_suffix = find_common_parts([word[::-1] for word in hapax_words], part_length)\n",
    "common_infix = find_common_parts(hapax_words, part_length)\n",
    "\n",
    "print(f'Most common prefix: {common_prefix}')\n",
    "print(f'Most common suffix: {common_suffix}')\n",
    "print(f'Most common infix: {common_infix}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability that a word ending with 'ing' is tagged as NOUN: 5.63%\n"
     ]
    }
   ],
   "source": [
    "# Define a function to calculate the probability\n",
    "def calculate_probability(word_tag_pairs):\n",
    "    total_count = len(word_tag_pairs)\n",
    "    ing_count = 0\n",
    "    \n",
    "    for word, tag in word_tag_pairs:\n",
    "        if word.endswith('ed') and tag == 'VERB':\n",
    "            ing_count += 1\n",
    "    \n",
    "    probability = ing_count / total_count\n",
    "    return probability\n",
    "\n",
    "# Read the file\n",
    "file_path = 'your_file.txt'  # Replace with the actual file path\n",
    "word_tag_pairs = []\n",
    "\n",
    "with open('hapax_word_tag_pairs.txt', 'r') as file:\n",
    "    for line in file:\n",
    "        try:\n",
    "            word,tag = line.strip().split(',')\n",
    "            word_tag_pairs.append((word, tag))\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "# Calculate the probability\n",
    "probability = calculate_probability(word_tag_pairs)\n",
    "\n",
    "print(f\"Probability that a word ending with 'ing' is tagged as NOUN: {probability:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability that words matching the pattern 'ation$' are tagged as NOUN: 1.00%\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Define a function to calculate the probability based on a given pattern\n",
    "def calculate_probability(word_tag_pairs, pattern, tag):\n",
    "    total_count = len(word_tag_pairs)\n",
    "    matching_count = 0\n",
    "    \n",
    "    for word, word_tag in word_tag_pairs:\n",
    "        if re.search(pattern, word) and word_tag == tag:\n",
    "            matching_count += 1\n",
    "    \n",
    "    probability = matching_count / total_count\n",
    "    return probability\n",
    "\n",
    "# Read the file\n",
    "file_path = 'your_file.txt'  # Replace with the actual file path\n",
    "word_tag_pairs = []\n",
    "\n",
    "with open('hapax_word_tag_pairs.txt', 'r') as file:\n",
    "    for line in file:\n",
    "        try:\n",
    "            word,tag = line.strip().split(',')\n",
    "            word_tag_pairs.append((word, tag))\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "# Define your pattern and desired tag\n",
    "your_pattern = r'ation$'  # This pattern matches words ending with 'ing'\n",
    "your_tag = 'NOUN'\n",
    "\n",
    "# Calculate the probability based on your pattern and tag\n",
    "probability = calculate_probability(word_tag_pairs, your_pattern, your_tag)\n",
    "\n",
    "print(f\"Probability that words matching the pattern '{your_pattern}' are tagged as {your_tag}: {probability:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Common Prefixes: {'DET': 'th', 'NOUN': 'co', 'ADJ': 'un', 'VERB': 're', 'IN': 'co', 'CONJ': 'su', 'ADV': 'un', 'PRON': 'th', 'NUM': '19', 'MODAL': 'sh', 'PART': 'af', 'UH': 'go', 'X': 'wh'}\n",
      "Common Suffixes: {'DET': 'er', 'NOUN': \"'s\", 'ADJ': 'ed', 'VERB': 'ed', 'IN': 'er', 'CONJ': 'ng', 'ADV': 'ly', 'PRON': \"'s\", 'NUM': '/2', 'MODAL': 'da', 'PART': 'er', 'UH': 'it', 'X': 'ed'}\n",
      "Common Infixes: {'DET': \"is'\", 'NOUN': 'a', 'ADJ': '-year-o', 'VERB': 'i', 'IN': 'th-but-aft', 'CONJ': 'pposi', 'ADV': 'ere', 'PRON': 'body', 'NUM': '-', 'MODAL': 'ul', 'PART': 't', 'UH': '-da-da-d', 'X': 'i'}\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def find_common_affixes(data, affix_length):\n",
    "    prefixes = defaultdict(list)\n",
    "    suffixes = defaultdict(list)\n",
    "    infixes = defaultdict(list)\n",
    "\n",
    "    for word, tag in data:\n",
    "        if len(word) > 2 * affix_length:\n",
    "            prefix = word[:affix_length]\n",
    "            suffix = word[-affix_length:]\n",
    "            infix = word[affix_length:-affix_length]\n",
    "\n",
    "            prefixes[tag].append(prefix)\n",
    "            suffixes[tag].append(suffix)\n",
    "            infixes[tag].append(infix)\n",
    "\n",
    "    common_prefixes = {tag: max(set(prefixes), key=prefixes.count) for tag, prefixes in prefixes.items()}\n",
    "    common_suffixes = {tag: max(set(suffixes), key=suffixes.count) for tag, suffixes in suffixes.items()}\n",
    "    common_infixes = {tag: max(set(infixes), key=infixes.count) for tag, infixes in infixes.items()}\n",
    "\n",
    "    return common_prefixes, common_suffixes, common_infixes\n",
    "\n",
    "# Your data goes here\n",
    "data = [('this\\'ll', 'DET'), ('das', 'DET'), ('tannenbaum', 'NOUN'), ('$.09', 'NOUN')]\n",
    "with open('hapax_word_tag_pairs.txt', 'r') as file:\n",
    "    for line in file:\n",
    "        try:\n",
    "            word,tag = line.strip().split(',')\n",
    "            data.append((word, tag))\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "common_prefixes, common_suffixes, common_infixes = find_common_affixes(data, affix_length=2)\n",
    "\n",
    "print(\"Common Prefixes:\", common_prefixes)\n",
    "print(\"Common Suffixes:\", common_suffixes)\n",
    "print(\"Common Infixes:\", common_infixes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
